{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to sparsify a Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lj9979/anaconda3/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model parameters count=83504416\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "from pytorch_block_sparse import BlockSparseModelPatcher\n",
    "import re\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config).cuda()\n",
    "\n",
    "# =>84 million parameters\n",
    "print(f\"Initial model parameters count={model.num_parameters()}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.self\\.query\n",
      "   => 768x768, bias=True\n",
      "roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.self\\.key\n",
      "   => 768x768, bias=True\n",
      "roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.self\\.value\n",
      "   => 768x768, bias=True\n",
      "roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.output\\.dense\n",
      "   => 768x768, bias=True\n",
      "roberta\\.encoder\\.layer\\.[0-9]+\\.intermediate\\.dense\n",
      "   => 768x3072, bias=True\n",
      "roberta\\.encoder\\.layer\\.[0-9]+\\.output\\.dense\n",
      "   => 3072x768, bias=True\n",
      "lm_head\\.dense\n",
      "   => 768x768, bias=True\n",
      "lm_head\\.decoder\n",
      "   => 768x52000, bias=True\n"
     ]
    }
   ],
   "source": [
    "# Create a model patcher\n",
    "mp = BlockSparseModelPatcher()\n",
    "\n",
    "# Show names that can be used: this returns a list of all names in the network that are patchable.\n",
    "# These names are escaped to be used as regexps in mp.add_pattern()\n",
    "patchables = mp.get_patchable_layers(model)\n",
    "\n",
    "dedup_layers = []\n",
    "\n",
    "# Pretty print the regexps: replace layer number with regexp matching numbers, and dedup them\n",
    "# This is a bit specific to Roberta, but should work for most transformers, it's just for ease of reading.\n",
    "for patchable in patchables:\n",
    "    r = patchable[\"regexp\"]\n",
    "    r = re.sub(r'[0-9]+', '[0-9]+', r)\n",
    "    if r not in dedup_layers:\n",
    "        dedup_layers.append(r)\n",
    "        layer = patchable['layer']\n",
    "        print(f\"{r}\\n   => {layer.in_features}x{layer.out_features}, bias={layer.bias is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching 'roberta.encoder.layer.0.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.0.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.0.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.1.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.1.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.1.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.2.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.2.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.2.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.3.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.3.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.3.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.4.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.4.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.4.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.5.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching 'roberta.encoder.layer.5.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching 'roberta.encoder.layer.5.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Final model parameters count=67579168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Selecting some layers to sparsify.\n",
    "# This is the \"artful\" part, as some parts are more prone to be sparsified, other may impact model precision too much.\n",
    "\n",
    "# Match layers using regexp (we escape the ., just because, it's more correct, but it does not change anything here)\n",
    "# the [0-9]+ match any layer number.\n",
    "# We setup a density of 0.5 on these layers, you can test other layers / densities .\n",
    "mp.add_pattern(\"roberta\\.encoder\\.layer\\.[0-9]+\\.intermediate\\.dense\", {\"density\":0.5})\n",
    "mp.add_pattern(\"roberta\\.encoder\\.layer\\.[0-9]+\\.output\\.dense\", {\"density\":0.5})\n",
    "mp.add_pattern(\"roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.output\\.dense\", {\"density\":0.5})\n",
    "mp.patch_model(model)\n",
    "\n",
    "print(f\"Final model parameters count={model.num_parameters()}\")\n",
    "\n",
    "# => 68 million parameters instead of 84 million parameters (embeddings are taking a lof space in Roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
